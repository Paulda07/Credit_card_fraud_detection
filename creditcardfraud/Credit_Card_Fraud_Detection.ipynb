{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PAUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data = pd.read_csv('C:/Users/PAUL/creditcardfraud/creditcard.csv')\n",
    "#print (cc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data = cc_data.sample(frac = 1)\n",
    "#shuffled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = pd.get_dummies(shuffled_data, columns = ['Class'])\n",
    "# one_hot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = (one_hot_data - one_hot_data.min())/(one_hot_data.max()-one_hot_data.min())\n",
    "# normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = normalized_data.drop(['Class_0','Class_1'], axis =1)\n",
    "# df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = normalized_data[['Class_0', 'Class_1']]\n",
    "# df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_X, ar_y = np.asarray(df_X, dtype = 'float32'), np.asarray(df_y, dtype = 'float32')\n",
    "# ar_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*len(ar_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw_X_train, raw_y_train) = (ar_X[:train_size], ar_y[:train_size])\n",
    "(raw_X_test, raw_y_test) = (ar_X[train_size:], ar_y[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of fraudlulent transactions: 0.001727485630620034\n"
     ]
    }
   ],
   "source": [
    "count_legit, count_fraud = np.unique(cc_data['Class'], return_counts=True)[1]\n",
    "fraud_ratio = float(count_fraud/(count_legit+count_fraud))\n",
    "print('Percent of fraudlulent transactions:',fraud_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting = 1/fraud_ratio\n",
    "raw_y_train[:,1] = raw_y_train[:,1]*weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = ar_X.shape[1]\n",
    "output_dim = ar_y.shape[1]\n",
    "num_layer1 = 100\n",
    "num_layer2 = 150\n",
    "\n",
    "X_train_node = tf.placeholder(tf.float32, [None, input_dim], name= 'X_train')\n",
    "y_train_node = tf.placeholder(tf.float32, [None, output_dim], name= 'y_train')\n",
    "\n",
    "X_test_node = tf.constant(raw_X_test, name = 'X_test')\n",
    "y_test_node = tf.constant(raw_y_test, name = 'y_test')\n",
    "\n",
    "\n",
    "weight_1_node = tf.Variable(tf.zeros([input_dim, num_layer1]), name = 'weight_1')\n",
    "biases_1_node = tf.Variable(tf.zeros([num_layer1]), name = 'bias_1')\n",
    "\n",
    "\n",
    "weight_2_node = tf.Variable(tf.zeros([num_layer1, num_layer2]), name = 'weight_2')\n",
    "biases_2_node = tf.Variable(tf.zeros([num_layer2]), name = 'bias_2')\n",
    "\n",
    "weight_3_node = tf.Variable(tf.zeros([num_layer2, output_dim]), name = 'weight_3')\n",
    "biases_3_node = tf.Variable(tf.zeros([output_dim]), name = 'bias_3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\paul\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def net(input):\n",
    "    layer1 = tf.nn.sigmoid(tf.matmul(input, weight_1_node)+biases_1_node)\n",
    "    layer2 = tf.nn.dropout(tf.nn.sigmoid(tf.matmul(layer1, weight_2_node)+biases_2_node), 0.85)\n",
    "    layer3 = tf.nn.softmax(tf.matmul(layer2,weight_3_node)+biases_3_node)\n",
    "    return layer3\n",
    "y_train_prediction = net(X_train_node)\n",
    "y_test_prediction = net(X_test_node)\n",
    "\n",
    "cross_entropy = tf.losses.softmax_cross_entropy(y_train_node, y_train_prediction)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.005).minimize(cross_entropy)\n",
    "\n",
    "def calc_accu(actual, predicted):\n",
    "    actual = np.argmax(actual, 1)\n",
    "    predicted = np.argmax(predicted, 1 )\n",
    "    return (100 * np.sum(np.equal(predicted, actual))/predicted.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Current loss 1.3948 Elapsed time: 4.24 seconds\n",
      "Epoch: 10 Current loss 1.3925 Elapsed time: 3.73 seconds\n",
      "Epoch: 20 Current loss 1.3669 Elapsed time: 3.84 seconds\n",
      "Epoch: 30 Current loss 1.2688 Elapsed time: 3.73 seconds\n",
      "Epoch: 40 Current loss 1.1022 Elapsed time: 3.80 seconds\n",
      "Epoch: 50 Current loss 0.9720 Elapsed time: 3.73 seconds\n",
      "Epoch: 60 Current loss 0.9097 Elapsed time: 3.77 seconds\n",
      "Epoch: 70 Current loss 0.8698 Elapsed time: 3.80 seconds\n",
      "Epoch: 80 Current loss 0.8530 Elapsed time: 3.75 seconds\n",
      "Epoch: 90 Current loss 0.8426 Elapsed time: 3.74 seconds\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-75ac442b9446>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mfinal_y_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mfinal_y_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mfinal_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_accu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_y_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_y_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Final Accuracy: {0:.2f}%'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-da778fa73e45>\u001b[0m in \u001b[0;36mcalc_accu\u001b[1;34m(actual, predicted)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        _, cross_entropy_score = session.run([optimizer, cross_entropy], feed_dict = {X_train_node: raw_X_train, y_train_node: raw_y_train})\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            timer = time.time()-start_time\n",
    "            \n",
    "            print ('Epoch: {}'. format(epoch), 'Current loss {0:.4f}'.format(cross_entropy_score), 'Elapsed time: {0:.2f} seconds'. format(timer))\n",
    "            \n",
    "    final_y_test = y_test_node.eval()\n",
    "    final_y_test_pred = y_test_prediction.eval()\n",
    "    final_accuracy = calc_accu(final_y_test, final_y_test_pred)\n",
    "    print ('Final Accuracy: {0:.2f}%'.format(final_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fraud_y_test = final_y_test[final_y_test[:,1]==1]\n",
    "final_fraud_y_test_pred = final_fraud_y_test_pred[final_y_test[:,1] == 1]\n",
    "final_fraud_accuracy = calc_accu(final_fraud_y_test, final_y_test_pred)\n",
    "print ('Final fraud specific Accuracy: {0:.2f}%'.format(final_fraud_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
